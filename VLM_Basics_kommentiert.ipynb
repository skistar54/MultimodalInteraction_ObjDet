{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# 1. OpenAI VLM (GPT) - Basics\n",
        "This section demonstrates the basic usage of OpenAI's Vision Language Model (VLM) capabilities using GPT-4.1.\n",
        "We will use the OpenAI API to analyze an image and provide detailed textual insights.\n",
        "\n",
        "**Support Material**\n",
        "\n",
        "- https://platform.openai.com/docs/quickstart\n",
        "- https://platform.openai.com/docs/guides/text\n",
        "- https://platform.openai.com/docs/guides/images-vision?api-mode=chat\n",
        "- https://platform.openai.com/docs/guides/structured-outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 1)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Es werden Hilfsbibliotheken geladen und eine Funktion `encode_image(...)` definiert.\n",
        "- - `encode_image` liest eine Bilddatei als Bytes ein und kodiert sie als Base64-String.\n",
        "- - Anschließend wird `.env` geladen, ein OpenAI-Client erstellt und ein Bildpfad gesetzt.\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Base64 wird genutzt, um ein lokales Bild als String direkt in einer API-Anfrage zu übertragen.\n",
        "- - Client + Bildpfad sind die Voraussetzung für spätere Vision-Calls (Text + Bild).\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Vorbereitung: Hilfsfunktion + Client/Inputs bereitstellen.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- Wenn `img`-Pfad falsch ist, schlägt das Einlesen/Kodieren später fehl.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "from dotenv import load_dotenv  \n",
        "import base64\n",
        "import json\n",
        "import textwrap\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')  # Bildbytes → Base64 kodieren\n",
        "\n",
        "\n",
        "load_dotenv()  # lädt Variablen aus .env\n",
        "openAIclient = openai.OpenAI()  # API-Client initialisieren\n",
        "\n",
        "\n",
        "# Path to your image\n",
        "img = \"images/street_scene.jpg\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 2)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Es wird eine Chat-Completion mit einem Vision-fähigen Modell aufgerufen.\n",
        "- - In `messages` wird Text + Bild gemeinsam als Input gesendet (multimodal).\n",
        "- - Die Antwort wird aus dem Response-Objekt als Text extrahiert und zur Lesbarkeit umbrochen ausgegeben.\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Typischer „Basic VLM Call“: schnell prüfen, ob das Modell das Bild korrekt beschreiben kann.\n",
        "- - Grundlage für spätere strukturierte Extraktion (z.B. JSON) oder Aufgaben wie Objekt-/Szenenbeschreibung.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "LLM/VLM-Call: Bild + Prompt → Modellantwort anzeigen.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- `openAIclient` und `img` müssen vorher gesetzt sein (sonst Fehler).\n",
        "- Response ist ein Objekt; relevant ist meist `choices[0].message.content`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The image shows a busy urban street scene with a mixture of people engaged in various activities. In the foreground,\n",
            "there is a young person sitting on the ground using a tablet or phone next to a potted plant with orange flowers.\n",
            "Nearby, another person is lying down on the pavement, wearing a red jacket and a beanie. On a bench, an older man in a\n",
            "suit is sitting next to a young woman with blonde hair, who is reading a newspaper.   In the background, there are cars\n",
            "moving through a crosswalk, a motorcyclist, a man walking and playing a guitar, and a scooter rider. A young woman is\n",
            "walking on the sidewalk while looking at her phone. Several pigeons are scattered on the ground near the bench and the\n",
            "person lying down.  The scene is set against a backdrop of tall buildings and city infrastructure, with a warm, late\n",
            "afternoon or early evening light. The image captures a vibrant, dynamic urban life moment with a mix of technology,\n",
            "transportation, and leisurely activities.\n"
          ]
        }
      ],
      "source": [
        "#basic call to gpt with prompt and image\n",
        "\n",
        "completion = openAIclient.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",  # ausgewähltes Modell\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",  # Bild als Base64-Data-URL mitsenden\n",
        "                    \"image_url\": {  # Bild als Base64-Data-URL mitsenden\n",
        "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
        "                        #\"detail\": \"low\"\n",
        "                    }\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "# Wrap the text to a specified width\n",
        "\n",
        "response = str(completion.choices[0].message.content)\n",
        "print(textwrap.fill(response, width=120))  # Ausgabe umbrechen (lesbarer)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# 1.1 Structured Output\n",
        "Here, we expand upon the VLM example to request structured outputs. This approach allows for extracting \n",
        "well-organized information from images in a machine-readable format, such as JSON.\n",
        "\n",
        "**Support Material**:\n",
        "- https://platform.openai.com/docs/guides/structured-outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 3)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Es wird erneut ein Vision-Call gemacht, diesmal mit System-Anweisung für vorsichtige Beobachtung.\n",
        "- - Mit `response_format` wird die Antwort explizit als JSON-Objekt angefordert.\n",
        "- - Der JSON-Text wird aus der Antwort herausgegriffen und in `returnValue` gespeichert.\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Strukturierte Outputs (JSON) sind im DS-Kontext leichter zu parsen, prüfen und weiterzuverarbeiten.\n",
        "- - Reduziert das Risiko von „freiem Text“, wenn du später auf einzelne Felder zugreifen willst.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "LLM/VLM-Call mit Formatvorgabe: strukturierte Bildbeschreibung erzeugen.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- Wenn das Modell kein gültiges JSON liefert, kann `json.loads(...)` später fehlschlagen.\n",
        "- `temperature=0` unterstützt reproduzierbarere, stabilere Struktur.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "completion = openAIclient.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",  # ausgewähltes Modell\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",  # Bild als Base64-Data-URL mitsenden\n",
        "                    \"image_url\": {  # Bild als Base64-Data-URL mitsenden\n",
        "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
        "                        #\"detail\": \"low\"\n",
        "                    }\n",
        "                },\n",
        "            ]}\n",
        "    ],\n",
        "    response_format={ \"type\": \"json_object\" },# NEW!!\n",
        "    temperature = 0  # Kreativität/Determinismus steuern\n",
        ")\n",
        "\n",
        "returnValue = completion.choices[0].message.content  # Antworttext aus Response extrahieren\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 4)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Der Code führt einen Zwischenschritt im Notebook aus (z.B. Aufruf, Parsing oder Anzeige).\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Unterstützt das iterative Experimentieren mit Vision-LLMs (Input → Output → prüfen).\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Zwischenschritt im Experiment-Workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\\n  \"scene\": \"Urban city street intersection during daytime\",\\n  \"background\": {\\n    \"buildings\": [\\n      {\\n        \"style\": \"Brick facade with green awnings\",\\n        \"windows\": \"Multiple, reflecting sunlight\",\\n        \"shops\": \"Visible with warm lighting inside\"\\n      },\\n      {\\n        \"style\": \"Modern glass skyscrapers\",\\n        \"height\": \"Tall, extending into the sky\",\\n        \"details\": \"Reflective surfaces with some trees in front\"\\n      },\\n      {\\n        \"church\": {\\n          \"architecture\": \"Traditional with a steeple\",\\n          \"location\": \"Center background\"\\n        }\\n      }\\n    ],\\n    \"traffic_light\": {\\n      \"color\": \"Yellow\",\\n      \"position\": \"Right side above the street\"\\n    }\\n  },\\n  \"street\": {\\n    \"crosswalk\": \"Wide zebra stripes\",\\n    \"vehicles\": [\\n      {\\n        \"type\": \"Taxi\",\\n        \"color\": \"White\",\\n        \"motion\": \"Blurred, indicating movement\"\\n      },\\n      {\\n        \"type\": \"SUV\",\\n        \"color\": \"Gray\",\\n        \"motion\": \"Moving\"\\n      },\\n      {\\n        \"type\": \"Sedan\",\\n        \"color\": \"Orange\",\\n        \"motion\": \"Moving\"\\n      }\\n    ],\\n    \"motorcycles\": [\\n      {\\n        \"rider\": \"Wearing black gear and white helmet\",\\n        \"motion\": \"Moving across crosswalk\"\\n      },\\n      {\\n        \"rider\": \"Wearing casual clothes and helmet\",\\n        \"vehicle\": \"Scooter\",\\n        \"motion\": \"Moving\"\\n      }\\n    ]\\n  },\\n  \"people\": [\\n    {\\n      \"location\": \"Left foreground\",\\n      \"activity\": \"Sitting cross-legged on sidewalk\",\\n      \"appearance\": \"Short hair, wearing a green jacket and shorts\",\\n      \"engagement\": \"Using a smartphone\"\\n    },\\n    {\\n      \"location\": \"Center foreground\",\\n      \"activity\": \"Lying on the ground\",\\n      \"appearance\": \"Red jacket, blue jeans, black shoes\",\\n      \"posture\": \"Relaxed, eyes closed\"\\n    },\\n    {\\n      \"location\": \"Center crossing\",\\n      \"activity\": \"Walking and playing guitar\",\\n      \"appearance\": \"Wearing a black jacket and cap\"\\n    },\\n    {\\n      \"location\": \"Right bench\",\\n      \"people\": [\\n        {\\n          \"appearance\": \"Older man in a suit\",\\n          \"activity\": \"Sitting, hand on chin, thoughtful\"\\n        },\\n        {\\n          \"appearance\": \"Young woman with short blonde hair\",\\n          \"clothing\": \"Red blouse and blue jeans\",\\n          \"activity\": \"Reading a newspaper\"\\n        }\\n      ]\\n    },\\n    {\\n      \"location\": \"Right sidewalk\",\\n      \"activity\": \"Walking while looking at phone\",\\n      \"appearance\": \"Long dark hair, pink top, denim shorts\"\\n    }\\n  ],\\n  \"animals\": {\\n    \"pigeons\": {\\n      \"count\": 7,\\n      \"location\": \"On the ground near people\",\\n      \"behavior\": \"Pecking and walking\"\\n    }\\n  },\\n  \"additional_elements\": {\\n    \"bench\": {\\n      \"material\": \"Wooden slats with black metal frame\",\\n      \"position\": \"Right foreground\"\\n    },\\n    \"plant\": {\\n      \"type\": \"Potted flowering plant\",\\n      \"location\": \"Left foreground near seated person\",\\n      \"flowers\": \"Red\"\\n    }\\n  },\\n  \"lighting\": \"Soft natural daylight with warm tones, sun low in the sky\",\\n  \"mood\": \"Busy urban life with a mix of relaxation and movement\"\\n}'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "returnValue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We parse the json in a dict structure:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 5)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Ein JSON-String wird in ein Python-Objekt (meist dict/list) umgewandelt.\n",
        "- - Anschließend wird das Objekt angezeigt/weiter genutzt.\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Damit man Felder gezielt auslesen kann (z.B. `output['background']...`).\n",
        "- - Ermöglicht einfache Weiterverarbeitung wie Auswertungen/Regeln/Alerts.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Auswertung: Antwort strukturieren, damit man damit arbeiten kann.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- Wenn der String kein valides JSON ist, gibt es einen Parsing-Fehler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'scene': 'Urban city street with a mix of pedestrians, vehicles, and street furniture', 'time_of_day': 'Late afternoon or early evening, indicated by the warm sunlight and long shadows', 'background': {'buildings': [{'style': 'Brick facade with large windows and green awnings', 'location': 'Left side of the street'}, {'style': 'Modern glass and steel skyscrapers', 'location': 'Center and right side, extending into the distance'}, {'style': 'Historic church-like building with a steeple', 'location': 'Center background'}], 'traffic_lights': {'color': 'Yellow', 'position': 'Hanging over the street on the right side'}, 'street': {'type': 'Crosswalk with white zebra stripes', 'vehicles': [{'type': 'Taxi', 'color': 'White', 'motion': 'Blurred, indicating movement', 'location': 'Left side of the crosswalk'}, {'type': 'SUV', 'color': 'Gray', 'motion': 'Moving', 'location': 'Behind the taxi'}, {'type': 'Sedan', 'color': 'Orange', 'motion': 'Moving', 'location': 'Center of the crosswalk'}], 'motorcycles': [{'rider': 'Wearing a black jacket and white helmet', 'motion': 'Moving across the crosswalk'}, {'rider': 'Wearing casual clothes and a helmet', 'vehicle': 'Scooter', 'motion': 'Moving'}]}}, 'foreground': {'people': [{'age': 'Young adult', 'gender': 'Male', 'clothing': 'Green jacket and shorts', 'activity': 'Sitting cross-legged on the sidewalk using a smartphone', 'location': 'Left foreground near a flower pot'}, {'age': 'Young adult', 'gender': 'Male', 'clothing': 'Red hoodie and jeans', 'activity': 'Lying on the sidewalk with eyes closed', 'location': 'Center foreground'}, {'age': 'Older adult', 'gender': 'Male', 'clothing': 'Gray suit', 'activity': 'Sitting on a bench, resting his head on his hand, appearing thoughtful', 'location': 'Right foreground'}, {'age': 'Young adult', 'gender': 'Female', 'clothing': 'Red blouse and blue jeans', 'activity': 'Sitting on the bench reading a newspaper', 'location': 'Right foreground next to the older man'}, {'age': 'Young adult', 'gender': 'Female', 'clothing': 'Pink top and denim shorts', 'activity': 'Walking while looking at a smartphone', 'location': 'Far right near the bench'}, {'age': 'Young adult', 'gender': 'Male', 'clothing': 'Black jacket and cap', 'activity': 'Walking across the street playing an acoustic guitar', 'location': 'Center crosswalk'}], 'animals': {'type': 'Pigeons', 'quantity': 7, 'location': 'Scattered on the sidewalk and near the bench'}, 'objects': [{'type': 'Wooden bench', 'color': 'Light brown with black metal frame', 'location': 'Right side of the sidewalk'}, {'type': 'Flower pot', 'contents': 'Green plant with red flowers', 'location': 'Left side near the sitting young man'}]}}\n"
          ]
        }
      ],
      "source": [
        "output = json.loads(returnValue)  # JSON-String → Python-Objekt\n",
        "#json. loads() converts JSON strings to Python objects\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we can access specific infos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 6)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Es wird ein konkretes Feld aus dem JSON-Objekt per Key/Index herausgegriffen.\n",
        "- - Das ausgewählte Detail wird ausgegeben.\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Prüft, ob die strukturierte Extraktion so aufgebaut ist wie erwartet.\n",
        "- - In RAG/VLM-Pipelines oft Schritt zur Validierung von Schema/Parsing.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Auswertung/Validierung: gezielt ein Ergebnisfeld prüfen.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- KeyError/IndexError, wenn das JSON-Schema anders ist als angenommen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Behind the taxi\n"
          ]
        }
      ],
      "source": [
        "print(output[\"background\"][\"street\"][\"vehicles\"][1][\"location\"])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# JSON Schema for Controlled Structured Outputs\n",
        "In this section, we define a JSON schema for a more controlled and specific output from the model. \n",
        "Using this schema, we can ensure the model adheres to predefined data types and structures while describing images.In this case we will provide the json schema directly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 7)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Der Code führt einen Zwischenschritt im Notebook aus (z.B. Aufruf, Parsing oder Anzeige).\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Unterstützt das iterative Experimentieren mit Vision-LLMs (Input → Output → prüfen).\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Zwischenschritt im Experiment-Workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "completion = openAIclient.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",  # ausgewähltes Modell\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",  # Bild als Base64-Data-URL mitsenden\n",
        "                    \"image_url\": {  # Bild als Base64-Data-URL mitsenden\n",
        "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
        "                        #\"detail\": \"low\"\n",
        "                    }\n",
        "                },\n",
        "            ]}\n",
        "    ],\n",
        "    response_format={  # Ausgabeformat erzwingen (JSON)\n",
        "                \"type\": \"json_schema\",    \n",
        "                \"json_schema\": {\n",
        "                    \"name\": \"img_extract\",\n",
        "                    \"schema\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"numberOfPeople\": {\n",
        "                        \"type\":\"integer\",\n",
        "                        \"description\": \"The total number of people in the environment\",\n",
        "                        \"minimum\": 0\n",
        "                        },\n",
        "                        \"atmosphere\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
        "                        },\n",
        "                        \"hourOfTheDay\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"The hour of the day in 24-hour format\",\n",
        "                        \"minimum\": 0,\n",
        "                        \"maximum\": 23\n",
        "                        },\n",
        "                        \"people\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"description\": \"List of people and their details\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"object\",\n",
        "                            \"properties\": {\n",
        "                            \"position\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
        "                            },\n",
        "                            \"age\": {\n",
        "                                \"type\": \"integer\",\n",
        "                                \"description\": \"Age of the person\",\n",
        "                                \"minimum\": 0\n",
        "                            },\n",
        "                            \"activity\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
        "                            },\n",
        "                            \"gender\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"Gender of the person\",\n",
        "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
        "                            }\n",
        "                            },\n",
        "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
        "                        }\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]\n",
        "                    }}},\n",
        "    temperature = 0  # Kreativität/Determinismus steuern\n",
        ")\n",
        "\n",
        "returnValue = completion.choices[0].message.content  # Antworttext aus Response extrahieren\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 8)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Ein JSON-String wird in ein Python-Objekt (meist dict/list) umgewandelt.\n",
        "- - Anschließend wird das Objekt angezeigt/weiter genutzt.\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Damit man Felder gezielt auslesen kann (z.B. `output['background']...`).\n",
        "- - Ermöglicht einfache Weiterverarbeitung wie Auswertungen/Regeln/Alerts.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Auswertung: Antwort strukturieren, damit man damit arbeiten kann.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- Wenn der String kein valides JSON ist, gibt es einen Parsing-Fehler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_image_extraction = json.loads(returnValue)  # JSON-String → Python-Objekt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 9)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Der Code führt einen Zwischenschritt im Notebook aus (z.B. Aufruf, Parsing oder Anzeige).\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Unterstützt das iterative Experimentieren mit Vision-LLMs (Input → Output → prüfen).\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Zwischenschritt im Experiment-Workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'position': 'sitting on the sidewalk',\n",
              "  'age': 15,\n",
              "  'activity': 'using a smartphone',\n",
              "  'gender': 'male'},\n",
              " {'position': 'lying on the sidewalk',\n",
              "  'age': 20,\n",
              "  'activity': 'resting or sleeping',\n",
              "  'gender': 'male'},\n",
              " {'position': 'sitting on a bench',\n",
              "  'age': 65,\n",
              "  'activity': 'thinking or resting',\n",
              "  'gender': 'male'},\n",
              " {'position': 'sitting on a bench',\n",
              "  'age': 25,\n",
              "  'activity': 'reading a newspaper',\n",
              "  'gender': 'female'},\n",
              " {'position': 'walking on the sidewalk',\n",
              "  'age': 20,\n",
              "  'activity': 'using a smartphone',\n",
              "  'gender': 'female'},\n",
              " {'position': 'riding a motorcycle',\n",
              "  'age': 30,\n",
              "  'activity': 'driving',\n",
              "  'gender': 'male'},\n",
              " {'position': 'walking on the street',\n",
              "  'age': 30,\n",
              "  'activity': 'playing guitar',\n",
              "  'gender': 'male'},\n",
              " {'position': 'riding a scooter',\n",
              "  'age': 25,\n",
              "  'activity': 'driving',\n",
              "  'gender': 'female'},\n",
              " {'position': 'walking on the sidewalk in the background',\n",
              "  'age': 30,\n",
              "  'activity': 'walking',\n",
              "  'gender': 'female'},\n",
              " {'position': 'walking on the sidewalk in the background',\n",
              "  'age': 30,\n",
              "  'activity': 'walking',\n",
              "  'gender': 'female'}]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_image_extraction[\"people\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively: \n",
        "\n",
        "\n",
        "OpenAI SDKs for Python and JavaScript also make it easy to define object schemas using Pydantic and Zod respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    position: str \n",
        "    age: int \n",
        "    activity: str \n",
        "    gender: str\n",
        "\n",
        "\n",
        "class ImageExtraction(BaseModel):\n",
        "    number_of_people: int \n",
        "    atmosphere: str \n",
        "    hour_of_the_day: int \n",
        "    people: list[Person] \n",
        "\n",
        "completion = openAIclient.beta.chat.completions.parse(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
        "        {\"role\": \"user\", \"content\": \"describe the image in detail\"}\n",
        "    ],\n",
        "    response_format=ImageExtraction,\n",
        ")\n",
        "\n",
        "output_image_extraction = completion.choices[0].message.parsed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then integrate the extracted information in full or partially in a new prompt for a new extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 10)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Es wird ein Prompt für einen nachgelagerten Schritt (z.B. Alert/Entscheidung) zusammengestellt.\n",
        "- - Die vorher extrahierten Bildinformationen werden in den Prompt eingebettet.\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Post-Processing: aus Roh-Extraktion werden handlungsorientierte Outputs (z.B. Warnungen, Zusammenfassungen).\n",
        "- - Typisch im DS-Kontext: Regeln/Business-Logik über LLM-Output legen.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Weiterverarbeitung: aus Extraktion wird ein Folge-Prompt/Task.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- Zu lange Prompts können Token-Limits auslösen; evtl. nur relevante Teile übergeben.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "#alert service prompt \n",
        "\n",
        "alert_sys_prompt = \" you are an experienced first aid paramedical\"\n",
        "alert_prompt= \"\"\"Extract from the following scene analysis give to you in json format, \n",
        "if anyone might be in danger and if the Child Hospital or normal Hospital should be alerted. \n",
        "Give the a concise answer\n",
        "The situation is given to you from this object: \"\"\" + str(output_image_extraction)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 11)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Es wird ein Prompt für einen nachgelagerten Schritt (z.B. Alert/Entscheidung) zusammengestellt.\n",
        "- - Die vorher extrahierten Bildinformationen werden in den Prompt eingebettet.\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Post-Processing: aus Roh-Extraktion werden handlungsorientierte Outputs (z.B. Warnungen, Zusammenfassungen).\n",
        "- - Typisch im DS-Kontext: Regeln/Business-Logik über LLM-Output legen.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Weiterverarbeitung: aus Extraktion wird ein Folge-Prompt/Task.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- Zu lange Prompts können Token-Limits auslösen; evtl. nur relevante Teile übergeben.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No one appears to be in immediate danger. The 20-year-old lying on the sidewalk seems to be resting or sleeping without\n",
            "signs of distress. No hospital alert is necessary. If needed, a normal hospital should be contacted rather than a child\n",
            "hospital.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "completion = openAIclient.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",  # ausgewähltes Modell\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": alert_prompt},\n",
        "        {\"role\": \"user\", \"content\": alert_prompt}\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "# Wrap the text to a specified width\n",
        "\n",
        "response = str(completion.choices[0].message.content)\n",
        "print(textwrap.fill(response, width=120))  # Ausgabe umbrechen (lesbarer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 12)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Es wird eine Chat-Completion mit einem Vision-fähigen Modell aufgerufen.\n",
        "- - In `messages` wird Text + Bild gemeinsam als Input gesendet (multimodal).\n",
        "- - Die Antwort wird aus dem Response-Objekt als Text extrahiert und zur Lesbarkeit umbrochen ausgegeben.\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Typischer „Basic VLM Call“: schnell prüfen, ob das Modell das Bild korrekt beschreiben kann.\n",
        "- - Grundlage für spätere strukturierte Extraktion (z.B. JSON) oder Aufgaben wie Objekt-/Szenenbeschreibung.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "LLM/VLM-Call: Bild + Prompt → Modellantwort anzeigen.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- `openAIclient` und `img` müssen vorher gesetzt sein (sonst Fehler).\n",
        "- Response ist ein Objekt; relevant ist meist `choices[0].message.content`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The youngest person according to the list is a 15-year-old male sitting on the sidewalk using a smartphone. In the\n",
            "provided image, the person corresponding to this description is the boy sitting at the bottom left corner of the image,\n",
            "holding a phone or tablet.  The approximate coordinates of this person in [ymin, xmin, ymax, xmax] format normalized to\n",
            "a 0-1000 scale would be:  [ymin: 635, xmin: 130, ymax: 880, xmax: 280]\n"
          ]
        }
      ],
      "source": [
        "completion = openAIclient.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",  # ausgewähltes Modell\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"Considering this list of people\"+str(output_image_extraction[\"people\"])+\".Identify the youngest in the picture I provide and give me back their coordinates. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",  # Bild als Base64-Data-URL mitsenden\n",
        "                    \"image_url\": {  # Bild als Base64-Data-URL mitsenden\n",
        "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
        "                        #\"detail\": \"low\"\n",
        "                    }\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "# Wrap the text to a specified width\n",
        "\n",
        "response = str(completion.choices[0].message.content)\n",
        "print(textwrap.fill(response, width=120))  # Ausgabe umbrechen (lesbarer)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# 2. Google VLM (Gemini)\n",
        "This section demonstrates the use of Google's Vision Language Model, Gemini. \n",
        "We explore basic text generation as well as its ability to analyze images and provide relevant outputs.\n",
        "\n",
        "**Support Material**:\n",
        "- https://ai.google.dev/gemini-api/docs/quickstart\n",
        "- https://ai.google.dev/gemini-api/docs/text-generation\n",
        "- https://ai.google.dev/gemini-api/docs/image-understanding\n",
        "- https://ai.google.dev/gemini-api/docs/structured-output?example=recipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 13)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- Imports & Setup: Es werden Bibliotheken eingebunden, die für VLM/LLM-Calls, Bildhandling und Ausgabe gebraucht werden.\n",
        "- `from dotenv import load_dotenv` — Umgebungsvariablen (.env) laden.\n",
        "- `from google import genai` — Gemini-API (Google GenAI).\n",
        "- `from PIL import Image` — Bilder laden/verarbeiten.\n",
        "- `import textwrap` — lange Ausgaben umbrechen.\n",
        "- `import json` — JSON einlesen/ausgeben.\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- Damit alle Bausteine bereitstehen, um Bilder + Text als Input an ein VLM zu schicken und strukturierte Outputs auszuwerten.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Setup: vorbereitet die Arbeitsumgebung, bevor Modellaufrufe und Auswertung passieren.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from dotenv import load_dotenv  \n",
        "from google import genai\n",
        "from PIL import Image\n",
        "import textwrap\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "load_dotenv()  # lädt Variablen aus .env\n",
        "client = genai.Client()\n",
        "\n",
        "# Path to your image\n",
        "img = \"images/street_scene.jpg\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Basic call:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 14)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Es wird ein Gemini-Client genutzt, um eine (Vision-)Anfrage zu senden.\n",
        "- - Textprompt und ggf. Bild werden als Input übergeben.\n",
        "- - Die Antwort wird ausgegeben (mit `textwrap` umbrochen).\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Zeigt, wie man Vision-Modelle auch über Gemini im Notebook testet.\n",
        "- - Gut für Modellvergleich: gleicher Task, anderer Provider.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "LLM/VLM-Call (Gemini): Prompt/Bild → Antwort.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- API-Key/Client muss korrekt konfiguriert sein; sonst Auth-Fehler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imagine a very smart computer helper.  You show it many, many examples, and it learns to recognize patterns so it can do\n",
            "things like understand your voice or spot faces in photos.\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(  # LLM/VLM-Call an Gemini\n",
        "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works to a 90 years old. in few words\"  # ausgewähltes Modell\n",
        ")\n",
        "\n",
        "print(textwrap.fill(response.text, width=120))  # Ausgabe umbrechen (lesbarer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and with images: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 15)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Es werden Bibliotheken für Bildverarbeitung/Visualisierung geladen und ein Bild geöffnet.\n",
        "- - Danach wird eine Gemini-Anfrage mit Bild + Textprompt vorbereitet/abgeschickt (falls enthalten).\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - VLM-Experimente brauchen Bild-Input; PIL/Matplotlib helfen beim Laden und Prüfen der Inputs.\n",
        "- - Visualisierung unterstützt Debugging (ist das richtige Bild geladen?).\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Vorbereitung/Debugging: Bild laden und ggf. für VLM-Call bereitstellen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This bustling urban scene captures a moment in a vibrant city during what appears to be late afternoon or early evening,\n",
            "bathed in the warm, golden light of a low sun. The image is rich with activity, showcasing a blend of architectural\n",
            "styles and diverse individuals going about their day.  In the **foreground**, a wide pedestrian crosswalk with bold\n",
            "black and white stripes diagonally cuts across the bottom left. On the sidewalk to the left, a small pot of red\n",
            "geraniums sits. Next to it, a young person with short brown hair sits cross-legged, engrossed in a tablet or phone.\n",
            "Further to the right and slightly in front, another young person, wearing a red hoodie and blue jeans, lies casually on\n",
            "their back on the pavement, looking upwards. Several pigeons are scattered on the sidewalk and crosswalk, pecking at the\n",
            "ground, adding to the authentic urban feel.  On the right side of the foreground, a classic wooden park bench is\n",
            "occupied by two individuals. An older man in a dark suit sits on the left of the bench, with one hand thoughtfully\n",
            "resting on his chin, looking off to the side. Beside him, a blonde woman in a striped reddish-orange shirt and blue\n",
            "jeans is deeply focused on reading a newspaper. She has a serene expression. Just past the bench, a young woman with\n",
            "long dark hair, wearing a pink t-shirt and denim shorts, walks gracefully with a small tray or plate in her hands,\n",
            "glancing towards the viewer.  The **midground** is dominated by the street and its various forms of transit. Several\n",
            "cars are captured in motion blur, indicating a busy thoroughfare. A silver car with a \"taxi\" sign on top streaks across\n",
            "the crosswalk from left to right. Behind it, a silver SUV is also in motion, and a gold-colored car is turning on the\n",
            "right side of the street. Crossing the street in front of the blurred vehicles, a man in a black leather suit and helmet\n",
            "rides a motorcycle. In the very center of the midground street, a man in a dark hat and coat strides purposefully,\n",
            "playing an acoustic guitar, seemingly unfazed by the surrounding traffic. Further right, a woman in a brown jacket and\n",
            "hat rides a scooter. Along the sidewalks in the midground, several other pedestrians can be seen walking, adding to the\n",
            "general hustle and bustle.  The **background** features a striking cityscape under a clear blue sky that transitions\n",
            "into a golden, hazy glow near the horizon, characteristic of sunset or sunrise. On the left, multi-story brick buildings\n",
            "with numerous windows and green awnings over storefronts create a historic feel, with warm light emanating from their\n",
            "interiors. In the center, a mix of modern glass skyscrapers towers over a beautiful, older church-like building with a\n",
            "pointed spire, creating an interesting contrast between old and new architecture. On the far right, another brick\n",
            "building with large windows also features an illuminated sign that appears to read \"LITTLE PEEK CAGES,\" though the text\n",
            "is somewhat stylized. A large traffic light hangs over the street, clearly displaying three red lights, suggesting that\n",
            "traffic is meant to stop despite the blurred moving vehicles.  Overall, the scene is dynamic and full of life, blending\n",
            "quiet contemplation with urban activity under a beautiful, warm light.\n"
          ]
        }
      ],
      "source": [
        "im = Image.open(img)  # Bilddatei öffnen\n",
        "\n",
        "response = client.models.generate_content(model=\"gemini-2.5-flash\",  # ausgewähltes Modell\n",
        "                                          contents=[im, \"Describe the scene in details\\n\"],\n",
        "                                          )\n",
        "\n",
        "print(textwrap.fill(response.text, width=120))  # Ausgabe umbrechen (lesbarer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also here we can extract structured output (Gemini actually prefers pydantic syntax - let's see what happens with a schema as before)-> check limitations in https://ai.google.dev/gemini-api/docs/structured-output?example=recipe "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 16)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Es wird ein Gemini-Client genutzt, um eine (Vision-)Anfrage zu senden.\n",
        "- - Textprompt und ggf. Bild werden als Input übergeben.\n",
        "- - Die Antwort wird ausgegeben (mit `textwrap` umbrochen).\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Zeigt, wie man Vision-Modelle auch über Gemini im Notebook testet.\n",
        "- - Gut für Modellvergleich: gleicher Task, anderer Provider.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "LLM/VLM-Call (Gemini): Prompt/Bild → Antwort.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- API-Key/Client muss korrekt konfiguriert sein; sonst Auth-Fehler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"visual_description\": \"A dynamic street scene in a bustling city, featuring a mix of people, vehicles, and architecture under a soft, golden hour light. The foreground shows a wide crosswalk where several individuals are engaged in various activities, while cars and motorcycles move past. Tall buildings, ranging from classic brick structures to modern skyscrapers, line the street, creating a deep urban perspective. Pigeons are scattered on the sidewalk, and a single potted plant adds a touch of nature.\",\n",
            "  \"elements\": [\n",
            "    \"crosswalk\",\n",
            "    \"traffic light\",\n",
            "    \"street lamps\",\n",
            "    \"buildings\",\n",
            "    \"skyscrapers\",\n",
            "    \"cars\",\n",
            "    \"motorcycle\",\n",
            "    \"scooter\",\n",
            "    \"benches\",\n",
            "    \"pigeons\",\n",
            "    \"potted plant with red flowers\",\n",
            "    \"sidewalk\"\n",
            "  ],\n",
            "  \"people\": [\n",
            "    \"A man in a dark jacket and helmet riding a motorcycle across the crosswalk.\",\n",
            "    \"A man in a dark jacket and hat walking and playing a guitar across the crosswalk.\",\n",
            "    \"A woman riding a scooter in the background, to the right.\",\n",
            "    \"An older man with glasses, wearing a dark suit, sitting on a wooden bench, appearing contemplative.\",\n",
            "    \"A blonde woman in a red and white striped top and jeans, sitting on the bench next to the older man, reading a newspaper.\",\n",
            "    \"A young woman in a pink t-shirt and light shorts, walking on the sidewalk to the right, holding a white plate or tray.\",\n",
            "    \"A young man in a green jacket and shorts, sitting cross-legged on the sidewalk near the crosswalk, looking at a tablet.\",\n",
            "    \"A young man in a red hoodie and jeans, lying on his back on the sidewalk near the center-bottom.\",\n",
            "    \"Several blurred pedestrians in the mid-ground and background, on the sidewalks.\"\n",
            "  ],\n",
            "  \"mood\": \"Busy, urban, vibrant, active, dynamic, everyday life\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "json_schema = {\n",
        "                    \"name\": \"img_extract\",\n",
        "                    \"schema\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"numberOfPeople\": {\n",
        "                        \"type\":\"integer\",\n",
        "                        \"description\": \"The total number of people in the environment\",\n",
        "                        \"minimum\": 0\n",
        "                        },\n",
        "                        \"atmosphere\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
        "                        },\n",
        "                        \"hourOfTheDay\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"The hour of the day in 24-hour format\",\n",
        "                        \"minimum\": 0,\n",
        "                        \"maximum\": 23\n",
        "                        },\n",
        "                        \"people\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"description\": \"List of people and their details\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"object\",\n",
        "                            \"properties\": {\n",
        "                            \"position\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
        "                            },\n",
        "                            \"age\": {\n",
        "                                \"type\": \"integer\",\n",
        "                                \"description\": \"Age of the person\",\n",
        "                                \"minimum\": 0\n",
        "                            },\n",
        "                            \"activity\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
        "                            },\n",
        "                            \"gender\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"Gender of the person\",\n",
        "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
        "                            }\n",
        "                            },\n",
        "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
        "                        }\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]}}\n",
        "\n",
        "\n",
        "\n",
        "config={\n",
        "        \"response_mime_type\": \"application/json\",\n",
        "        \"response_json_schema\": json_schema,\n",
        "    }\n",
        "\n",
        "\n",
        "response = client.models.generate_content(model=\"gemini-2.5-flash\",  # ausgewähltes Modell\n",
        "                                          contents=[im, \"Describe the scene in details, follwoing exactly the given json schema\\n\"],\n",
        "                                          config=config\n",
        "                                          )\n",
        "\n",
        "\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Does it match your schema?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try to use Gemini to detect an object in the image and get its coordinates:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 17)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Ein JSON-String wird in ein Python-Objekt (meist dict/list) umgewandelt.\n",
        "- - Anschließend wird das Objekt angezeigt/weiter genutzt.\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Damit man Felder gezielt auslesen kann (z.B. `output['background']...`).\n",
        "- - Ermöglicht einfache Weiterverarbeitung wie Auswertungen/Regeln/Alerts.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Auswertung: Antwort strukturieren, damit man damit arbeiten kann.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- Wenn der String kein valides JSON ist, gibt es einen Parsing-Fehler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'box_2d': [664, 461, 794, 513]}\n",
            "{\n",
            "  \"box_2d\": [664, 461, 794, 513]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Identify the youngest in the picture and give me back their coordinates. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"\n",
        "\n",
        "\n",
        "config={\"response_mime_type\": \"application/json\"}\n",
        "\n",
        "response = client.models.generate_content(model=\"gemini-2.5-flash\",  # ausgewähltes Modell\n",
        "                                          contents=[img, prompt],\n",
        "                                          config=config\n",
        "                                          )\n",
        "\n",
        "bounding_boxes = json.loads(response.text)  # JSON-String → Python-Objekt\n",
        "print(bounding_boxes)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gemini2+ was trained specifically for object detection/ segmentation tasks. More details: https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.Extract Structured Infos from Hand-written note - GPT & Gemini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s try **not** to extract structured information from a handwritten note (e.g., `prescription1.jpg`) using **both models**.\n",
        "\n",
        "Consider the file: `/images/prescription1.jpg`.  \n",
        "Have a look at it.\n",
        "\n",
        "### JSON Schema\n",
        "Let’s define a JSON schema for the extraction task:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 18)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Der Code führt einen Zwischenschritt im Notebook aus (z.B. Aufruf, Parsing oder Anzeige).\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Unterstützt das iterative Experimentieren mit Vision-LLMs (Input → Output → prüfen).\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Zwischenschritt im Experiment-Workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "json_schema_prescription = {\n",
        " \"name\": \"prescription_extract\",\n",
        "\"schema\": {\n",
        "  \"type\": \"object\",\n",
        "  \"properties\": {\n",
        "    \"doctor_name\": { \"type\": \"string\" },\n",
        "    \"patient_name\": { \"type\": \"string\" },\n",
        "    \"patient_dob\": { \"type\": \"string\" },\n",
        "    \"meds\": {\n",
        "      \"type\": \"array\",\n",
        "      \"items\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"name\": { \"type\": \"string\" },\n",
        "          \"dose\": { \"type\": \"string\" },\n",
        "          \"frequency\": { \"type\": \"string\" },\n",
        "          \"instructions\": { \"type\": \"string\" }\n",
        "        },\n",
        "        \"required\": [\"name\"]\n",
        "      }\n",
        "    },\n",
        "    \"signature\": { \"type\": \"boolean\" }\n",
        "  },\n",
        "  \"required\": [\"doctor_name\", \"patient_name\", \"meds\"]\n",
        "}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract structured infos using Gemini: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 19)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Es werden Bibliotheken für Bildverarbeitung/Visualisierung geladen und ein Bild geöffnet.\n",
        "- - Danach wird eine Gemini-Anfrage mit Bild + Textprompt vorbereitet/abgeschickt (falls enthalten).\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - VLM-Experimente brauchen Bild-Input; PIL/Matplotlib helfen beim Laden und Prüfen der Inputs.\n",
        "- - Visualisierung unterstützt Debugging (ist das richtige Bild geladen?).\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Vorbereitung/Debugging: Bild laden und ggf. für VLM-Call bereitstellen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"doctor\": \"Dr. Markus Müller\",\n",
            "  \"patient\": \"Claudle Fischer\",\n",
            "  \"dateOfBirth\": \"01.04.1978\",\n",
            "  \"gender\": \"Female\",\n",
            "  \"medication\": [\n",
            "    \"Ibuprofen\",\n",
            "    \"3x 400mg\",\n",
            "    \"nach dem Essen\"\n",
            "  ],\n",
            "  \"signature\": \"Reptuller\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "im = Image.open(\"images/prescription1.jpg\")  # Bilddatei öffnen\n",
        "\n",
        "config={\n",
        "        \"response_mime_type\": \"application/json\",\n",
        "        \"response_json_schema\": json_schema_prescription,\n",
        "    }\n",
        "\n",
        "\n",
        "response = client.models.generate_content(model=\"gemini-2.5-flash\",  # ausgewähltes Modell\n",
        "                                          contents=[im, \"Extract infos from image, follwoing the given json schema.\\n\"],\n",
        "                                          config=config\n",
        "                                          )\n",
        "\n",
        "\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the output is **not valid JSON** and contains extra strings, it must be **parsed** before it can be loaded into a Python dict.  \n",
        "Below is an example helper function that does this.\n",
        "\n",
        "> **Note:** Since Gemini returns a Pydantic model, you *could* use Pydantic methods to handle parsing.  \n",
        "> We avoid that here to keep the workflow generally compatible across models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 20)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Ein JSON-String wird in ein Python-Objekt (meist dict/list) umgewandelt.\n",
        "- - Anschließend wird das Objekt angezeigt/weiter genutzt.\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Damit man Felder gezielt auslesen kann (z.B. `output['background']...`).\n",
        "- - Ermöglicht einfache Weiterverarbeitung wie Auswertungen/Regeln/Alerts.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Auswertung: Antwort strukturieren, damit man damit arbeiten kann.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- Wenn der String kein valides JSON ist, gibt es einen Parsing-Fehler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import json \n",
        "def parse_json_in_output(output):\n",
        "    \"\"\"\n",
        "    Extracts and converts JSON-like data from the given text output to a Python dictionary.\n",
        "    \n",
        "    Args:\n",
        "        output (str): The text output containing the JSON data.\n",
        "    \n",
        "    Returns:\n",
        "        dict: The parsed JSON data as a Python dictionary.\n",
        "    \"\"\"\n",
        "    # Regex to extract JSON-like portion\n",
        "    json_match = re.search(r\"\\{.*?\\}\", output, re.DOTALL)\n",
        "    if json_match:\n",
        "        json_str = json_match.group(0)\n",
        "        # Fix single quotes and ensure proper JSON formatting\n",
        "        json_str = json_str.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
        "        try:\n",
        "            # Convert the fixed JSON string into a dictionary\n",
        "            json_data = json.loads(json_str)  # JSON-String → Python-Objekt\n",
        "            return json_data\n",
        "        except json.JSONDecodeError:\n",
        "            return \"The extracted JSON is still not valid after formatting.\"\n",
        "    else:\n",
        "        return \"No JSON data found in the given output.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 21)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Der Code führt einen Zwischenschritt im Notebook aus (z.B. Aufruf, Parsing oder Anzeige).\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Unterstützt das iterative Experimentieren mit Vision-LLMs (Input → Output → prüfen).\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Zwischenschritt im Experiment-Workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(parse_json_in_output(response.text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 22)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Ein JSON-String wird in ein Python-Objekt (meist dict/list) umgewandelt.\n",
        "- - Anschließend wird das Objekt angezeigt/weiter genutzt.\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Damit man Felder gezielt auslesen kann (z.B. `output['background']...`).\n",
        "- - Ermöglicht einfache Weiterverarbeitung wie Auswertungen/Regeln/Alerts.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Auswertung: Antwort strukturieren, damit man damit arbeiten kann.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- Wenn der String kein valides JSON ist, gibt es einen Parsing-Fehler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'patientName': 'Claudie Fischer',\n",
              " 'doctorName': 'Dr. Markus Müller',\n",
              " 'medications': ['Ibuprofen', '400mg', '3x', 'nach dem Essen'],\n",
              " 'dateOfBirth': '01.04.1978',\n",
              " 'gender': 'f',\n",
              " 'diagnosis': None,\n",
              " 'signature': 'Reichmüller'}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "json.loads(response.text)  # JSON-String → Python-Objekt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's do the same with GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 23)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Der Code führt einen Zwischenschritt im Notebook aus (z.B. Aufruf, Parsing oder Anzeige).\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Unterstützt das iterative Experimentieren mit Vision-LLMs (Input → Output → prüfen).\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Zwischenschritt im Experiment-Workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "im = \"images/prescription1.jpg\"\n",
        "\n",
        "completion = openAIclient.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",  # ausgewähltes Modell\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",  # Bild als Base64-Data-URL mitsenden\n",
        "                    \"image_url\": {  # Bild als Base64-Data-URL mitsenden\n",
        "                        \"url\": f\"data:image/jpeg;base64,{encode_image(im)}\",\n",
        "                        #\"detail\": \"low\" -> je tiefer desto weniger tokens werden verwendet\n",
        "                    }\n",
        "                },\n",
        "            ]}\n",
        "    ],\n",
        "    response_format={  # Ausgabeformat erzwingen (JSON)\n",
        "                \"type\": \"json_schema\",   \"json_schema\": json_schema_prescription},\n",
        "    temperature = 0  # Kreativität/Determinismus steuern\n",
        ")\n",
        "\n",
        "returnValue = completion.choices[0].message.content  # Antworttext aus Response extrahieren\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 24)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Der Code führt einen Zwischenschritt im Notebook aus (z.B. Aufruf, Parsing oder Anzeige).\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Unterstützt das iterative Experimentieren mit Vision-LLMs (Input → Output → prüfen).\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Zwischenschritt im Experiment-Workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\"doctor_name\":\"Dr. Markus Müller\",\"patient_name\":\"Claudia Fischer\",\"patient_dob\":\"1.4.1978\",\"meds\":[{\"name\":\"Ibuprofen\",\"dose\":\"400 mg\",\"frequency\":\"3x\",\"instructions\":\"nach dem Essen\"}],\"signature\":true}'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "returnValue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Any difference wiht the output of Gemini vs your schema? \n",
        "\n",
        "No need for parsing now. We load the json in a python dict structure with json.loads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Erklärung zu Code-Block (Zelle 25)\n",
        "\n",
        "**Was passiert hier?**\n",
        "- - Ein JSON-String wird in ein Python-Objekt (meist dict/list) umgewandelt.\n",
        "- - Anschließend wird das Objekt angezeigt/weiter genutzt.\n",
        "\n",
        "**Warum macht man das? (Data-Science/LLM-Kontext)**\n",
        "- - Damit man Felder gezielt auslesen kann (z.B. `output['background']...`).\n",
        "- - Ermöglicht einfache Weiterverarbeitung wie Auswertungen/Regeln/Alerts.\n",
        "\n",
        "**Rolle im Gesamtprozess**\n",
        "Auswertung: Antwort strukturieren, damit man damit arbeiten kann.\n",
        "\n",
        "**Wichtig / typische Stolpersteine (optional)**\n",
        "- Wenn der String kein valides JSON ist, gibt es einen Parsing-Fehler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'doctor_name': 'Dr. Markus Müller', 'patient_name': 'Claudia Fischer', 'patient_dob': '1.4.1978', 'meds': [{'name': 'Ibuprofen', 'dose': '400 mg', 'frequency': '3x', 'instructions': 'nach dem Essen'}], 'signature': True}\n"
          ]
        }
      ],
      "source": [
        "print(json.loads(returnValue))  # JSON-String → Python-Objekt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zusammenfassung für die Prüfung (max. 40 Zeilen)\n",
        "\n",
        "Dieses Notebook zeigt die Grundidee von **Vision-Language Models (VLMs)**: Du kombinierst **Text (Prompt)** und **Bild** als Input, schickst beides an ein Modell (z.B. über OpenAI oder Gemini), bekommst ein **Response-Objekt** zurück und extrahierst daraus den eigentlichen Text/Output.\n",
        "\n",
        "Der typische Ablauf ist: **Setup** (Imports, API-Key via `.env`, Client) → **Input vorbereiten** (Bildpfad laden, Bild in Base64 oder als Objekt übergeben) → **Model-Call** (`chat.completions.create` / `generate_content`) → **Output nutzen** (anzeigen, in JSON erzwingen, parsen, gezielt Felder auslesen).\n",
        "\n",
        "Für Prüfungen wichtig: **Prompt ≠ Antwort**. Der Prompt enthält Ziel, Kontext und Formatvorgaben (z.B. „gib JSON zurück“). Unterschiedliche Prompts führen zu unterschiedlichen Antworten – deshalb testet man Prompts iterativ. Wenn im Call ein Chat-Verlauf (`messages`) verwendet wird, beeinflussen Rollen und Kontext (system/user/assistant) die Antwort.\n",
        "\n",
        "Im Data-Science-Kontext ist die **Auswertung** zentral: Outputs sind nicht automatisch „wahr“. Du prüfst Plausibilität, Konsistenz und Schema (z.B. ob JSON wirklich valide ist). Für Weiterverarbeitung ist strukturierter Output (JSON) oft besser als Fließtext.\n",
        "\n",
        "Typische Stolpersteine: fehlende API-Keys/ENV (Client kann nicht authentifizieren), falsche Modellnamen, **Response-Objekt vs. Text** (wo liegt der Inhalt?), sowie Parsing-Probleme bei JSON (`json.loads` schlägt fehl, wenn kein valides JSON zurückkommt). Auch Token-/Längenlimits können Antworten abschneiden.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mögliche Prüfungsfragen zu diesem Notebook (Einschätzung)\n",
        "\n",
        "1. Erkläre den Zweck von `load_dotenv()` und warum API-Keys nicht direkt im Notebook stehen sollten.\n",
        "2. Warum wird ein Bild für den API-Call oft als Base64-String kodiert? Was passiert dabei grob?\n",
        "3. Erkläre den Aufbau von `messages`: Wie werden Text und Bild zusammen an das Modell übergeben?\n",
        "4. Was ist der Unterschied zwischen einem „freien“ Bild-Describe-Call und einem Call mit `response_format={\"type\":\"json_object\"}`?\n",
        "5. Warum kann `json.loads(...)` fehlschlagen, obwohl das Modell „JSON“ liefern sollte?\n",
        "6. Wo findest du im Response-Objekt typischerweise den eigentlichen Antworttext?\n",
        "7. Was bewirkt `temperature=0` in solchen Extraktionsaufgaben?\n",
        "8. Nenne typische Risiken bei VLM-Ausgaben (z.B. Halluzinationen) und wie du sie im Workflow prüfst.\n",
        "9. Was würde passieren, wenn der Bildpfad `img` falsch ist oder die Datei fehlt?\n",
        "10. Wie würdest du Prompt A vs. Prompt B vergleichen, wenn beide ein JSON-Schema erfüllen sollen?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}