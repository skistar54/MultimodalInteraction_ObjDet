{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Object Positions in Images - YOLO vs VLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the image: `/images/table_scene.jpeg`.\n",
    "\n",
    "### TASK\n",
    "Detect the cup on the table and identify it visually by **drawing a bounding box around it on the image itself**.  \n",
    "You can do this using the provided `utils` functions, or by writing your own.\n",
    "\n",
    "### Models to Compare\n",
    "Compare the performance of these three models:\n",
    "\n",
    "1. **YOLO**\n",
    "2. **GPT-4.1-mini**\n",
    "3. **Gemini 2.5-flash**\n",
    "\n",
    "### Notes\n",
    "Depending on your prompt choice and the modelâ€™s output format, you may need to:\n",
    "\n",
    "- update the `utils` functions, or  \n",
    "- adapt your code to make it compatible (e.g., **parsing outputs** that come back as text vs JSON).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils \n",
    "Some of the utils are taken and modified for the purpose of the exercise from \n",
    "https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing#scrollTo=wizbxA1lm-Tj\n",
    "https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Object_detection.ipynb#scrollTo=245bc92a470f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from PIL import Image, ImageDraw\n",
    "from PIL import ImageColor\n",
    "\n",
    "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n",
    "\n",
    "#this function is needed to plot bounding boxes on images \n",
    "def plot_bounding_boxes(im, positions):\n",
    "    \"\"\"\n",
    "    Plots bounding boxes on an image with markers for each noun phrase, using PIL, normalized coordinates, and different colors.\n",
    "\n",
    "    Args:\n",
    "        img_path: The path to the image file.\n",
    "        noun_phrases_and_positions: A list of tuples containing the noun phrases\n",
    "         and their positions in normalized [y1 x1 y2 x2] format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the image\n",
    "    img = im\n",
    "    width, height = img.size\n",
    "    print(img.size)\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Define a list of colors\n",
    "    colors = [\n",
    "    'red',\n",
    "    'green',\n",
    "    'blue',\n",
    "    'yellow',\n",
    "    'orange',\n",
    "    ] + additional_colors\n",
    "\n",
    "    # Iterate over the noun phrases and their positions\n",
    "    for i, ((y1, x1, y2, x2)) in enumerate(positions):\n",
    "        # Select a color from the list\n",
    "        color = colors[i % len(colors)]\n",
    "\n",
    "        # Convert normalized coordinates to absolute coordinates\n",
    "        abs_x1 = int(x1/1000 * width)\n",
    "        abs_y1 = int(y1/1000 * height)\n",
    "        abs_x2 = int(x2/1000 * width)\n",
    "        abs_y2 = int(y2/1000 * height)\n",
    "\n",
    "        # Draw the bounding box\n",
    "        draw.rectangle(\n",
    "            ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4\n",
    "        )\n",
    "\n",
    "        # Draw the text\n",
    "        #draw.text((abs_x1 + 8, abs_y1 + 6), noun_phrase, fill=color)\n",
    "\n",
    "    # Display the image\n",
    "    img.show()\n",
    "\n",
    "# if the boxes coordinates are output not as json but as text, should be parsed first\n",
    "def parse_list_boxes(text):\n",
    "  result = []\n",
    "  for line in text.strip().splitlines():\n",
    "    # Extract the numbers from the line, remove brackets and split by comma\n",
    "    try:\n",
    "      numbers = line.split('[')[1].split(']')[0].split(',')\n",
    "    except:\n",
    "      numbers =  line.split('- ')[1].split(',')\n",
    "\n",
    "    # Convert the numbers to integers and append to the result\n",
    "    result.append([int(num.strip()) for num in numbers])\n",
    "\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 14) (196088658.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    text_labels = [[\"Trafficlight]]\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 14)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "\n",
    "image = Image.open(\"images/street_scene.jpg\")\n",
    "\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "\n",
    "\n",
    "text_labels = [[\"Trafficlight]]\n",
    "\n",
    "inputs = processor(text=text_labels, images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "target_sizes = torch.tensor([(image.height, image.width)])\n",
    "\n",
    "# Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels\n",
    ")\n",
    "# Retrieve predictions for the first image for the corresponding text queries\n",
    "result = results[0]\n",
    "boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n",
    "\n",
    "for box, score, text_label in zip(boxes, scores, text_labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    print(box)\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
    "    )\n",
    "\n",
    "\n",
    "def show_boxes_and_labels_on_image(raw_image, boxes, labels, scores):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(raw_image)\n",
    "    for i, box in enumerate(boxes):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        show_box(box, plt.gca())\n",
    "        plt.text(\n",
    "            x=box[0],\n",
    "            y=box[1] - 12,\n",
    "            s=f\"{labels[i]}: {scores[i]:,.4f}\",\n",
    "            c=\"beige\",\n",
    "            path_effects=[pe.withStroke(linewidth=4, foreground=\"darkgreen\")],\n",
    "        )\n",
    "    plt.axis(\"on\")\n",
    "    plt.show()\n",
    "    #lt.savefig(\"streetscene_with_detections.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "# Show the image with the bounding boxes\n",
    "show_boxes_and_labels_on_image(\n",
    "    image,\n",
    "    boxes,\n",
    "    text_labels,\n",
    "    scores\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
