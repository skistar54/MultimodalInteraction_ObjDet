{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. OpenAI VLM (GPT) - Basics\n",
    "This section demonstrates the basic usage of OpenAI's Vision Language Model (VLM) capabilities using GPT-4.1.\n",
    "We will use the OpenAI API to analyze an image and provide detailed textual insights.\n",
    "\n",
    "**Support Material**\n",
    "\n",
    "- https://platform.openai.com/docs/quickstart\n",
    "- https://platform.openai.com/docs/guides/text\n",
    "- https://platform.openai.com/docs/guides/images-vision?api-mode=chat\n",
    "- https://platform.openai.com/docs/guides/structured-outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import base64\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openAIclient = openai.OpenAI()\n",
    "\n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image depicts a lively urban street scene at a crosswalk in a city with tall buildings. On the foreground sidewalk,\n",
      "there is a wooden bench where an older man and a young woman are seated, with the woman reading a newspaper. A young\n",
      "person is sitting on the ground engrossed in a tablet, and another person is lying down on the pavement nearby. Several\n",
      "pigeons are scattered around the ground.   On the street, there's moderate traffic including a taxi, a car, a person on\n",
      "a motorcycle, a person riding a scooter, and a street musician walking and playing guitar. A young woman dressed in\n",
      "shorts and a pink top is crossing the street near the bench. The scene captures a mix of people engaging in different\n",
      "activities in a bustling city environment during what appears to be late afternoon or early evening.\n"
     ]
    }
   ],
   "source": [
    "#basic call to gpt with prompt and image\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1 Structured Output\n",
    "Here, we expand upon the VLM example to request structured outputs. This approach allows for extracting \n",
    "well-organized information from images in a machine-readable format, such as JSON.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/structured-outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={ \"type\": \"json_object\" },# NEW!!\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"scene\": \"Urban city street intersection during daytime\",\\n  \"background\": {\\n    \"buildings\": [\\n      {\\n        \"style\": \"Brick facade with green awnings\",\\n        \"windows\": \"Multiple, reflecting sunlight\",\\n        \"shops\": \"Visible with warm lighting inside\"\\n      },\\n      {\\n        \"style\": \"Modern glass skyscrapers\",\\n        \"height\": \"Tall, extending into the sky\",\\n        \"reflection\": \"Sky and surrounding buildings\"\\n      },\\n      {\\n        \"style\": \"Historic church-like building\",\\n        \"features\": \"Steeple with a pointed roof\",\\n        \"location\": \"Center background\"\\n      }\\n    ],\\n    \"traffic_light\": {\\n      \"color\": \"Yellow\",\\n      \"position\": \"Hanging over the intersection\"\\n    },\\n    \"street\": {\\n      \"crosswalk\": \"Wide zebra stripes\",\\n      \"vehicles\": [\\n        {\\n          \"type\": \"Taxi\",\\n          \"color\": \"White\",\\n          \"motion\": \"Blurred, indicating movement\"\\n        },\\n        {\\n          \"type\": \"SUV\",\\n          \"color\": \"Gray\",\\n          \"motion\": \"Moving\"\\n        },\\n        {\\n          \"type\": \"Sedan\",\\n          \"color\": \"Orange\",\\n          \"motion\": \"Moving\"\\n        }\\n      ]\\n    }\\n  },\\n  \"foreground\": {\\n    \"people\": [\\n      {\\n        \"position\": \"Left side, sitting on the ground\",\\n        \"activity\": \"Using a tablet\",\\n        \"clothing\": \"Green jacket, shorts, sneakers\"\\n      },\\n      {\\n        \"position\": \"Center, lying on the ground\",\\n        \"clothing\": \"Red hoodie, blue jeans, black sneakers\",\\n        \"posture\": \"Relaxed, looking upwards\"\\n      },\\n      {\\n        \"position\": \"Center, crossing street\",\\n        \"activity\": \"Playing guitar\",\\n        \"clothing\": \"Black jacket, black pants, black cap\"\\n      },\\n      {\\n        \"position\": \"Center, riding motorcycle\",\\n        \"clothing\": \"Black outfit, white helmet\"\\n      },\\n      {\\n        \"position\": \"Center-right, riding scooter\",\\n        \"clothing\": \"Casual, helmet\"\\n      },\\n      {\\n        \"position\": \"Right side, sitting on bench\",\\n        \"people\": [\\n          {\\n            \"age\": \"Older man\",\\n            \"clothing\": \"Gray suit\",\\n            \"activity\": \"Thinking or resting with hand on face\"\\n          },\\n          {\\n            \"age\": \"Young woman\",\\n            \"clothing\": \"Red blouse, blue jeans\",\\n            \"activity\": \"Reading a newspaper\"\\n          }\\n        ]\\n      },\\n      {\\n        \"position\": \"Far right, walking\",\\n        \"clothing\": \"Pink top, denim shorts, white sneakers\",\\n        \"activity\": \"Looking at phone\"\\n      }\\n    ],\\n    \"animals\": {\\n      \"type\": \"Pigeons\",\\n      \"number\": 7,\\n      \"location\": \"On the ground near people\"\\n    },\\n    \"objects\": [\\n      {\\n        \"type\": \"Wooden bench\",\\n        \"color\": \"Light brown\",\\n        \"location\": \"Right side\"\\n      },\\n      {\\n        \"type\": \"Flower pot\",\\n        \"contents\": \"Green plant with red flowers\",\\n        \"location\": \"Left side near sitting person\"\\n      }\\n    ]\\n  },\\n  \"lighting\": \"Soft, warm sunlight illuminating the scene from the background\",\\n  \"mood\": \"Calm and lively urban atmosphere with a mix of relaxation and movement\"\\n}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse the json in a dict structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scene': 'Urban city street intersection during daytime', 'background': {'buildings': [{'style': 'Brick facade with green awnings', 'windows': 'Multiple, reflecting sunlight', 'shops': 'Visible with warm lighting inside'}, {'style': 'Modern glass skyscrapers', 'height': 'Tall, extending into the sky', 'reflection': 'Sky and surrounding buildings'}, {'style': 'Historic church-like building', 'features': 'Steeple with a pointed roof', 'location': 'Center background'}], 'traffic_light': {'color': 'Yellow', 'position': 'Hanging over the intersection'}, 'street': {'crosswalk': 'Wide zebra stripes', 'vehicles': [{'type': 'Taxi', 'color': 'White', 'motion': 'Blurred, indicating movement'}, {'type': 'SUV', 'color': 'Gray', 'motion': 'Moving'}, {'type': 'Sedan', 'color': 'Orange', 'motion': 'Moving'}]}}, 'foreground': {'people': [{'position': 'Left side, sitting on the ground', 'activity': 'Using a tablet', 'clothing': 'Green jacket, shorts, sneakers'}, {'position': 'Center, lying on the ground', 'clothing': 'Red hoodie, blue jeans, black sneakers', 'posture': 'Relaxed, looking upwards'}, {'position': 'Center, crossing street', 'activity': 'Playing guitar', 'clothing': 'Black jacket, black pants, black cap'}, {'position': 'Center, riding motorcycle', 'clothing': 'Black outfit, white helmet'}, {'position': 'Center-right, riding scooter', 'clothing': 'Casual, helmet'}, {'position': 'Right side, sitting on bench', 'people': [{'age': 'Older man', 'clothing': 'Gray suit', 'activity': 'Thinking or resting with hand on face'}, {'age': 'Young woman', 'clothing': 'Red blouse, blue jeans', 'activity': 'Reading a newspaper'}]}, {'position': 'Far right, walking', 'clothing': 'Pink top, denim shorts, white sneakers', 'activity': 'Looking at phone'}], 'animals': {'type': 'Pigeons', 'number': 7, 'location': 'On the ground near people'}, 'objects': [{'type': 'Wooden bench', 'color': 'Light brown', 'location': 'Right side'}, {'type': 'Flower pot', 'contents': 'Green plant with red flowers', 'location': 'Left side near sitting person'}]}, 'lighting': 'Soft, warm sunlight illuminating the scene from the background', 'mood': 'Calm and lively urban atmosphere with a mix of relaxation and movement'}\n"
     ]
    }
   ],
   "source": [
    "output = json.loads(returnValue)\n",
    "#json. loads() converts JSON strings to Python objects\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can access specific infos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'location'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackground\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstreet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvehicles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'location'"
     ]
    }
   ],
   "source": [
    "print(output[\"background\"][\"street\"][\"vehicles\"][1][\"location\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# JSON Schema for Controlled Structured Outputs\n",
    "In this section, we define a JSON schema for a more controlled and specific output from the model. \n",
    "Using this schema, we can ensure the model adheres to predefined data types and structures while describing images.In this case we will provide the json schema directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={\n",
    "                \"type\": \"json_schema\",    \n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\":\"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]\n",
    "                    }}},\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_extraction = json.loads(returnValue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'position': 'sitting on the sidewalk',\n",
       "  'age': 15,\n",
       "  'activity': 'using a smartphone',\n",
       "  'gender': 'male'},\n",
       " {'position': 'lying on the sidewalk',\n",
       "  'age': 20,\n",
       "  'activity': 'resting or sleeping',\n",
       "  'gender': 'male'},\n",
       " {'position': 'sitting on a bench',\n",
       "  'age': 65,\n",
       "  'activity': 'thinking or resting',\n",
       "  'gender': 'male'},\n",
       " {'position': 'sitting on a bench',\n",
       "  'age': 25,\n",
       "  'activity': 'reading a newspaper',\n",
       "  'gender': 'female'},\n",
       " {'position': 'walking on the sidewalk',\n",
       "  'age': 20,\n",
       "  'activity': 'using a smartphone',\n",
       "  'gender': 'female'},\n",
       " {'position': 'riding a motorcycle',\n",
       "  'age': 30,\n",
       "  'activity': 'driving',\n",
       "  'gender': 'male'},\n",
       " {'position': 'walking on the street',\n",
       "  'age': 30,\n",
       "  'activity': 'playing guitar',\n",
       "  'gender': 'male'},\n",
       " {'position': 'riding a scooter',\n",
       "  'age': 25,\n",
       "  'activity': 'driving',\n",
       "  'gender': 'female'},\n",
       " {'position': 'walking on the sidewalk in the background',\n",
       "  'age': 30,\n",
       "  'activity': 'walking',\n",
       "  'gender': 'female'},\n",
       " {'position': 'walking on the sidewalk in the background',\n",
       "  'age': 30,\n",
       "  'activity': 'walking',\n",
       "  'gender': 'female'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_image_extraction[\"people\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively: \n",
    "\n",
    "\n",
    "OpenAI SDKs for Python and JavaScript also make it easy to define object schemas using Pydantic and Zod respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    position: str \n",
    "    age: int \n",
    "    activity: str \n",
    "    gender: str\n",
    "\n",
    "\n",
    "class ImageExtraction(BaseModel):\n",
    "    number_of_people: int \n",
    "    atmosphere: str \n",
    "    hour_of_the_day: int \n",
    "    people: list[Person] \n",
    "\n",
    "completion = openAIclient.beta.chat.completions.parse(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": \"describe the image in detail\"}\n",
    "    ],\n",
    "    response_format=ImageExtraction,\n",
    ")\n",
    "\n",
    "output_image_extraction = completion.choices[0].message.parsed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then integrate the extracted information in full or partially in a new prompt for a new extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alert service prompt \n",
    "\n",
    "alert_sys_prompt = \" you are an experienced first aid paramedical\"\n",
    "alert_prompt= \"\"\"Extract from the following scene analysis give to you in json format, \n",
    "if anyone might be in danger and if the Child Hospital or normal Hospital should be alerted. \n",
    "Give the a concise answer\n",
    "The situation is given to you from this object: \"\"\" + str(output_image_extraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No one appears to be in immediate danger. The 20-year-old lying on the sidewalk seems to be resting or sleeping without\n",
      "signs of distress. No hospital alert is necessary. If needed, a normal hospital should be contacted rather than a child\n",
      "hospital.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": alert_prompt},\n",
    "        {\"role\": \"user\", \"content\": alert_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The youngest person according to the list is a 15-year-old male sitting on the sidewalk using a smartphone. In the\n",
      "provided image, the person corresponding to this description is the boy sitting at the bottom left corner of the image,\n",
      "holding a phone or tablet.  The approximate coordinates of this person in [ymin, xmin, ymax, xmax] format normalized to\n",
      "a 0-1000 scale would be:  [ymin: 635, xmin: 130, ymax: 880, xmax: 280]\n"
     ]
    }
   ],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Considering this list of people\"+str(output_image_extraction[\"people\"])+\".Identify the youngest in the picture I provide and give me back their coordinates. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Google VLM (Gemini)\n",
    "This section demonstrates the use of Google's Vision Language Model, Gemini. \n",
    "We explore basic text generation as well as its ability to analyze images and provide relevant outputs.\n",
    "\n",
    "**Support Material**:\n",
    "- https://ai.google.dev/gemini-api/docs/quickstart\n",
    "- https://ai.google.dev/gemini-api/docs/text-generation\n",
    "- https://ai.google.dev/gemini-api/docs/image-understanding\n",
    "- https://ai.google.dev/gemini-api/docs/structured-output?example=recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from dotenv import load_dotenv  \n",
    "from google import genai\n",
    "from PIL import Image\n",
    "import textwrap\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "client = genai.Client()\n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine a very smart computer helper.  You show it many, many examples, and it learns to recognize patterns so it can do\n",
      "things like understand your voice or spot faces in photos.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works to a 90 years old. in few words\"\n",
    ")\n",
    "\n",
    "print(textwrap.fill(response.text, width=120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This bustling urban scene captures a moment in a vibrant city during what appears to be late afternoon or early evening,\n",
      "bathed in the warm, golden light of a low sun. The image is rich with activity, showcasing a blend of architectural\n",
      "styles and diverse individuals going about their day.  In the **foreground**, a wide pedestrian crosswalk with bold\n",
      "black and white stripes diagonally cuts across the bottom left. On the sidewalk to the left, a small pot of red\n",
      "geraniums sits. Next to it, a young person with short brown hair sits cross-legged, engrossed in a tablet or phone.\n",
      "Further to the right and slightly in front, another young person, wearing a red hoodie and blue jeans, lies casually on\n",
      "their back on the pavement, looking upwards. Several pigeons are scattered on the sidewalk and crosswalk, pecking at the\n",
      "ground, adding to the authentic urban feel.  On the right side of the foreground, a classic wooden park bench is\n",
      "occupied by two individuals. An older man in a dark suit sits on the left of the bench, with one hand thoughtfully\n",
      "resting on his chin, looking off to the side. Beside him, a blonde woman in a striped reddish-orange shirt and blue\n",
      "jeans is deeply focused on reading a newspaper. She has a serene expression. Just past the bench, a young woman with\n",
      "long dark hair, wearing a pink t-shirt and denim shorts, walks gracefully with a small tray or plate in her hands,\n",
      "glancing towards the viewer.  The **midground** is dominated by the street and its various forms of transit. Several\n",
      "cars are captured in motion blur, indicating a busy thoroughfare. A silver car with a \"taxi\" sign on top streaks across\n",
      "the crosswalk from left to right. Behind it, a silver SUV is also in motion, and a gold-colored car is turning on the\n",
      "right side of the street. Crossing the street in front of the blurred vehicles, a man in a black leather suit and helmet\n",
      "rides a motorcycle. In the very center of the midground street, a man in a dark hat and coat strides purposefully,\n",
      "playing an acoustic guitar, seemingly unfazed by the surrounding traffic. Further right, a woman in a brown jacket and\n",
      "hat rides a scooter. Along the sidewalks in the midground, several other pedestrians can be seen walking, adding to the\n",
      "general hustle and bustle.  The **background** features a striking cityscape under a clear blue sky that transitions\n",
      "into a golden, hazy glow near the horizon, characteristic of sunset or sunrise. On the left, multi-story brick buildings\n",
      "with numerous windows and green awnings over storefronts create a historic feel, with warm light emanating from their\n",
      "interiors. In the center, a mix of modern glass skyscrapers towers over a beautiful, older church-like building with a\n",
      "pointed spire, creating an interesting contrast between old and new architecture. On the far right, another brick\n",
      "building with large windows also features an illuminated sign that appears to read \"LITTLE PEEK CAGES,\" though the text\n",
      "is somewhat stylized. A large traffic light hangs over the street, clearly displaying three red lights, suggesting that\n",
      "traffic is meant to stop despite the blurred moving vehicles.  Overall, the scene is dynamic and full of life, blending\n",
      "quiet contemplation with urban activity under a beautiful, warm light.\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Describe the scene in details\\n\"],\n",
    "                                          )\n",
    "\n",
    "print(textwrap.fill(response.text, width=120))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here we can extract structured output (Gemini actually prefers pydantic syntax - let's see what happens with a schema as before)-> check limitations in https://ai.google.dev/gemini-api/docs/structured-output?example=recipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"visual_description\": \"A dynamic street scene in a bustling city, featuring a mix of people, vehicles, and architecture under a soft, golden hour light. The foreground shows a wide crosswalk where several individuals are engaged in various activities, while cars and motorcycles move past. Tall buildings, ranging from classic brick structures to modern skyscrapers, line the street, creating a deep urban perspective. Pigeons are scattered on the sidewalk, and a single potted plant adds a touch of nature.\",\n",
      "  \"elements\": [\n",
      "    \"crosswalk\",\n",
      "    \"traffic light\",\n",
      "    \"street lamps\",\n",
      "    \"buildings\",\n",
      "    \"skyscrapers\",\n",
      "    \"cars\",\n",
      "    \"motorcycle\",\n",
      "    \"scooter\",\n",
      "    \"benches\",\n",
      "    \"pigeons\",\n",
      "    \"potted plant with red flowers\",\n",
      "    \"sidewalk\"\n",
      "  ],\n",
      "  \"people\": [\n",
      "    \"A man in a dark jacket and helmet riding a motorcycle across the crosswalk.\",\n",
      "    \"A man in a dark jacket and hat walking and playing a guitar across the crosswalk.\",\n",
      "    \"A woman riding a scooter in the background, to the right.\",\n",
      "    \"An older man with glasses, wearing a dark suit, sitting on a wooden bench, appearing contemplative.\",\n",
      "    \"A blonde woman in a red and white striped top and jeans, sitting on the bench next to the older man, reading a newspaper.\",\n",
      "    \"A young woman in a pink t-shirt and light shorts, walking on the sidewalk to the right, holding a white plate or tray.\",\n",
      "    \"A young man in a green jacket and shorts, sitting cross-legged on the sidewalk near the crosswalk, looking at a tablet.\",\n",
      "    \"A young man in a red hoodie and jeans, lying on his back on the sidewalk near the center-bottom.\",\n",
      "    \"Several blurred pedestrians in the mid-ground and background, on the sidewalks.\"\n",
      "  ],\n",
      "  \"mood\": \"Busy, urban, vibrant, active, dynamic, everyday life\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_schema = {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\":\"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]}}\n",
    "\n",
    "\n",
    "\n",
    "config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": json_schema,\n",
    "    }\n",
    "\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Describe the scene in details, follwoing exactly the given json schema\\n\"],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it match your schema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use Gemini to detect an object in the image and get its coordinates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'box_2d': [664, 461, 794, 513]}\n",
      "{\n",
      "  \"box_2d\": [664, 461, 794, 513]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Identify the youngest in the picture and give me back their coordinates. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"\n",
    "\n",
    "\n",
    "config={\"response_mime_type\": \"application/json\"}\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[img, prompt],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "bounding_boxes = json.loads(response.text)\n",
    "print(bounding_boxes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini2+ was trained specifically for object detection/ segmentation tasks. More details: https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Extract Structured Infos from Hand-written note - GPT & Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try **not** to extract structured information from a handwritten note (e.g., `prescription1.jpg`) using **both models**.\n",
    "\n",
    "Consider the file: `/images/prescription1.jpg`.  \n",
    "Have a look at it.\n",
    "\n",
    "### JSON Schema\n",
    "Let’s define a JSON schema for the extraction task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema_prescription = {\n",
    " \"name\": \"prescription_extract\",\n",
    "\"schema\": {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"doctor_name\": { \"type\": \"string\" },\n",
    "    \"patient_name\": { \"type\": \"string\" },\n",
    "    \"patient_dob\": { \"type\": \"string\" },\n",
    "    \"meds\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"name\": { \"type\": \"string\" },\n",
    "          \"dose\": { \"type\": \"string\" },\n",
    "          \"frequency\": { \"type\": \"string\" },\n",
    "          \"instructions\": { \"type\": \"string\" }\n",
    "        },\n",
    "        \"required\": [\"name\"]\n",
    "      }\n",
    "    },\n",
    "    \"signature\": { \"type\": \"boolean\" }\n",
    "  },\n",
    "  \"required\": [\"doctor_name\", \"patient_name\", \"meds\"]\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract structured infos using Gemini: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"doctor\": \"Dr. Markus Müller\",\n",
      "  \"patient\": \"Claudle Fischer\",\n",
      "  \"dateOfBirth\": \"01.04.1978\",\n",
      "  \"gender\": \"Female\",\n",
      "  \"medication\": [\n",
      "    \"Ibuprofen\",\n",
      "    \"3x 400mg\",\n",
      "    \"nach dem Essen\"\n",
      "  ],\n",
      "  \"signature\": \"Reptuller\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"images/prescription1.jpg\")\n",
    "\n",
    "config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": json_schema_prescription,\n",
    "    }\n",
    "\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Extract infos from image, follwoing the given json schema.\\n\"],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output is **not valid JSON** and contains extra strings, it must be **parsed** before it can be loaded into a Python dict.  \n",
    "Below is an example helper function that does this.\n",
    "\n",
    "> **Note:** Since Gemini returns a Pydantic model, you *could* use Pydantic methods to handle parsing.  \n",
    "> We avoid that here to keep the workflow generally compatible across models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json \n",
    "def parse_json_in_output(output):\n",
    "    \"\"\"\n",
    "    Extracts and converts JSON-like data from the given text output to a Python dictionary.\n",
    "    \n",
    "    Args:\n",
    "        output (str): The text output containing the JSON data.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The parsed JSON data as a Python dictionary.\n",
    "    \"\"\"\n",
    "    # Regex to extract JSON-like portion\n",
    "    json_match = re.search(r\"\\{.*?\\}\", output, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(0)\n",
    "        # Fix single quotes and ensure proper JSON formatting\n",
    "        json_str = json_str.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
    "        try:\n",
    "            # Convert the fixed JSON string into a dictionary\n",
    "            json_data = json.loads(json_str)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError:\n",
    "            return \"The extracted JSON is still not valid after formatting.\"\n",
    "    else:\n",
    "        return \"No JSON data found in the given output.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(parse_json_in_output(response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patientName': 'Claudie Fischer',\n",
       " 'doctorName': 'Dr. Markus Müller',\n",
       " 'medications': ['Ibuprofen', '400mg', '3x', 'nach dem Essen'],\n",
       " 'dateOfBirth': '01.04.1978',\n",
       " 'gender': 'f',\n",
       " 'diagnosis': None,\n",
       " 'signature': 'Reichmüller'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = \"images/prescription1.jpg\"\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(im)}\",\n",
    "                        #\"detail\": \"low\" -> je tiefer desto weniger tokens werden verwendet\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={\n",
    "                \"type\": \"json_schema\",   \"json_schema\": json_schema_prescription},\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"doctor_name\":\"Dr. Markus Müller\",\"patient_name\":\"Claudia Fischer\",\"patient_dob\":\"1.4.1978\",\"meds\":[{\"name\":\"Ibuprofen\",\"dose\":\"400 mg\",\"frequency\":\"3x\",\"instructions\":\"nach dem Essen\"}],\"signature\":true}'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any difference wiht the output of Gemini vs your schema? \n",
    "\n",
    "No need for parsing now. We load the json in a python dict structure with json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doctor_name': 'Dr. Markus Müller', 'patient_name': 'Claudia Fischer', 'patient_dob': '1.4.1978', 'meds': [{'name': 'Ibuprofen', 'dose': '400 mg', 'frequency': '3x', 'instructions': 'nach dem Essen'}], 'signature': True}\n"
     ]
    }
   ],
   "source": [
    "print(json.loads(returnValue))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
